'<!doctype html><html xmlns="http://www.w3.org/1999/xhtml" xmlns:o="urn:schemas-microsoft-com:office:office"><head><!--[if gte mso 9]><xml><o:OfficeDocumentSettings><o:AllowPNG/><o:PixelsPerInch>96</o:PixelsPerInch></o:OfficeDocumentSettings></xml><![endif]--><style>body{background-color:#fff}.gse_alrt_title{text-decoration:none}.gse_alrt_title:hover{text-decoration:underline} @media screen and (max-width: 599px) {.gse_alrt_sni br{display:none;}}</style></head><body><!--[if gte mso 9]><table cellpadding="0" cellspacing="0" border="0"><tr><td style="width:600px"><![endif]--><div style="font-family:arial,sans-serif;font-size:13px;line-height:16px;color:#222;width:100%;max-width:600px"><h3 style="font-weight:lighter;font-size:18px;line-height:20px;"></h3><h3 style="font-weight:normal;font-size:18px;line-height:20px;"></h3><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2405.14554&amp;hl=en&amp;sa=X&amp;d=14262674559806912548&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeZoT1cABoza4DLeDxDJ8kND&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=0&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">UDKAG: Augmenting Large Vision-Language Models with Up-to-Date Knowledge</a></h3><div style="color:#006621;line-height:18px">C Li, Z Li, C Jing, S Liu, W Shao, Y Wu, P Luo, Y Qiao…\xa0- arXiv preprint arXiv\xa0…, 2024</div><div class="gse_alrt_sni" style="line-height:17px">Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, <br>such as LLaVA series, because they cannot be updated frequently due to the large <br>amount of resources required, and therefore fail in many cases. For example, if a\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=JLhK_PEy78UJ&amp;citsig=AIIUsnMAAAAAaEIZ0hNPMzkoDFbZo5ljWS-wCUg" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2405.14554&amp;rt=UDKAG:+Augmenting+Large+Vision-Language+Models+with+Up-to-Date+Knowledge&amp;scisig=AFWwaealBGWmDcG6z5EreygTrBDu" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2405.14554&amp;rt=UDKAG:+Augmenting+Large+Vision-Language+Models+with+Up-to-Date+Knowledge&amp;scisig=AFWwaealBGWmDcG6z5EreygTrBDu" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2405.14554&amp;rt=UDKAG:+Augmenting+Large+Vision-Language+Models+with+Up-to-Date+Knowledge&amp;scisig=AFWwaealBGWmDcG6z5EreygTrBDu" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2405.15973&amp;hl=en&amp;sa=X&amp;d=4194908983169282830&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeanjcq9gSDXhsox9uSxaeUq&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=1&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement</a></h3><div style="color:#006621;line-height:18px">X Wang, J Chen, Z Wang, Y Zhou, Y Zhou, H Yao…\xa0- arXiv preprint arXiv\xa0…, 2024</div><div class="gse_alrt_sni" style="line-height:17px">Large vision-language models (LVLMs) have achieved impressive results in various <br>visual question-answering and reasoning tasks through vision instruction tuning on <br>specific datasets. However, there is still significant room for improvement in the\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=DnPmnn5PNzoJ&amp;citsig=AIIUsnMAAAAAaEIZ0qKCFLHn8V9CAs91me0pt2I" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2405.15973&amp;rt=Enhancing+Visual-Language+Modality+Alignment+in+Large+Vision+Language+Models+via+Self-Improvement&amp;scisig=AFWwaebhRU01e5zlji3S5lAoWMqJ" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2405.15973&amp;rt=Enhancing+Visual-Language+Modality+Alignment+in+Large+Vision+Language+Models+via+Self-Improvement&amp;scisig=AFWwaebhRU01e5zlji3S5lAoWMqJ" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2405.15973&amp;rt=Enhancing+Visual-Language+Modality+Alignment+in+Large+Vision+Language+Models+via+Self-Improvement&amp;scisig=AFWwaebhRU01e5zlji3S5lAoWMqJ" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2405.17423&amp;hl=en&amp;sa=X&amp;d=11772436576668871452&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeanGgrirDtyjJOXdbDhieeP&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=2&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Privacy-Aware Visual Language Models</a></h3><div style="color:#006621;line-height:18px">L Samson, N Barazani, S Ghebreab, YM Asano\xa0- arXiv preprint arXiv:2405.17423, 2024</div><div class="gse_alrt_sni" style="line-height:17px">This paper aims to advance our understanding of how Visual Language Models <br>(VLMs) handle privacy-sensitive information, a crucial concern as these technologies <br>become integral to everyday life. To this end, we introduce a new benchmark\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=HOsphbEYYKMJ&amp;citsig=AIIUsnMAAAAAaEIZ0pRzDWhDs-7szGTlr2Zz9NY" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2405.17423&amp;rt=Privacy-Aware+Visual+Language+Models&amp;scisig=AFWwaeaaul7Dn92dh_oQZYOSlEFz" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2405.17423&amp;rt=Privacy-Aware+Visual+Language+Models&amp;scisig=AFWwaeaaul7Dn92dh_oQZYOSlEFz" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2405.17423&amp;rt=Privacy-Aware+Visual+Language+Models&amp;scisig=AFWwaeaaul7Dn92dh_oQZYOSlEFz" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2405.18711&amp;hl=en&amp;sa=X&amp;d=17979828480406994110&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeaBwd9GAebjZ9URCyALaWwd&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=3&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Calibrating Reasoning in Language Models with Internal Consistency</a></h3><div style="color:#006621;line-height:18px">Z Xie, J Guo, T Yu, S Li\xa0- arXiv preprint arXiv:2405.18711, 2024</div><div class="gse_alrt_sni" style="line-height:17px">Large language models (LLMs) have demonstrated impressive capabilities in <br>various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting <br>that elicits verbalized reasoning. However, LLMs often generate text with obvious\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=vjwc4b0uhfkJ&amp;citsig=AIIUsnMAAAAAaEIZ0jtHCOlSQvuwFmkKP_m3JCM" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2405.18711&amp;rt=Calibrating+Reasoning+in+Language+Models+with+Internal+Consistency&amp;scisig=AFWwaeZtDdUYtmg-oN8tkVzBoBAR" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2405.18711&amp;rt=Calibrating+Reasoning+in+Language+Models+with+Internal+Consistency&amp;scisig=AFWwaeZtDdUYtmg-oN8tkVzBoBAR" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2405.18711&amp;rt=Calibrating+Reasoning+in+Language+Models+with+Internal+Consistency&amp;scisig=AFWwaeZtDdUYtmg-oN8tkVzBoBAR" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><a href="https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-97-3076-6_16&amp;hl=en&amp;sa=X&amp;d=7612677789412912463&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeb1IUcqzAv94nquVz5m65Ld&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=4&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Improving robustness in language models for legal textual entailment through artifact-aware training</a></h3><div style="color:#006621;line-height:18px">S Wehnert, V Murugadas, PV Naik, EW De Luca\xa0- JSAI International Symposium on\xa0…, 2024</div><div class="gse_alrt_sni" style="line-height:17px">In this paper, we describe our participation in COLIEE 2024, focusing on legal textual <br>entailment (Tasks 2 and 4). Our goal is to address language artifacts during <br>language model training for improved robustness. Limited domain-specific datasets\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=T212ox2qpWkJ&amp;citsig=AIIUsnMAAAAAaEIZ0ggPyQ0tyl49eNwJwxEfyzs" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://link.springer.com/chapter/10.1007/978-981-97-3076-6_16&amp;rt=Improving+robustness+in+language+models+for+legal+textual+entailment+through+artifact-aware+training&amp;scisig=AFWwaeYWZkEc1NAzjeMDp_ewFYDh" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://link.springer.com/chapter/10.1007/978-981-97-3076-6_16&amp;rt=Improving+robustness+in+language+models+for+legal+textual+entailment+through+artifact-aware+training&amp;scisig=AFWwaeYWZkEc1NAzjeMDp_ewFYDh" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://link.springer.com/chapter/10.1007/978-981-97-3076-6_16&amp;rt=Improving+robustness+in+language+models+for+legal+textual+entailment+through+artifact-aware+training&amp;scisig=AFWwaeYWZkEc1NAzjeMDp_ewFYDh" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2405.14159&amp;hl=en&amp;sa=X&amp;d=15362061592709538738&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaebqsnBXOBs_g3G7OM2lOd4e&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=5&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Super Tiny Language Models</a></h3><div style="color:#006621;line-height:18px">D Hillier, L Guertler, C Tan, P Agrawal, C Ruirui…\xa0- arXiv preprint arXiv\xa0…, 2024</div><div class="gse_alrt_sni" style="line-height:17px">The rapid advancement of large language models (LLMs) has led to significant <br>improvements in natural language processing but also poses challenges due to their <br>high computational and energy demands. This paper introduces a series of research\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=sqMfe6ABMdUJ&amp;citsig=AIIUsnMAAAAAaEIZ0rKD8q3ccjpOBFKAULOSw-M" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2405.14159&amp;rt=Super+Tiny+Language+Models&amp;scisig=AFWwaeaGw20jyOgfIuMMvdNm85o7" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2405.14159&amp;rt=Super+Tiny+Language+Models&amp;scisig=AFWwaeaGw20jyOgfIuMMvdNm85o7" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2405.14159&amp;rt=Super+Tiny+Language+Models&amp;scisig=AFWwaeaGw20jyOgfIuMMvdNm85o7" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2405.15356&amp;hl=en&amp;sa=X&amp;d=8604771668682789638&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeb4d432gkCGtGxZKI2XpGgO&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=6&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization</a></h3><div style="color:#006621;line-height:18px">B Chen, X Lyu, L Gao, J Song, HT Shen\xa0- arXiv preprint arXiv:2405.15356, 2024</div><div class="gse_alrt_sni" style="line-height:17px">Although Large Visual Language Models (LVLMs) have demonstrated exceptional <br>abilities in understanding multimodal data, they invariably suffer from hallucinations, <br>leading to a disconnect between the generated text and the corresponding images\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=BtMuHT5KancJ&amp;citsig=AIIUsnMAAAAAaEIZ0iTI0WQ0e5l_eBLR8dQDQZo" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2405.15356&amp;rt=Alleviating+Hallucinations+in+Large+Vision-Language+Models+through+Hallucination-Induced+Optimization&amp;scisig=AFWwaeaHmXVMl_CTTxu5IVbaGU3N" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2405.15356&amp;rt=Alleviating+Hallucinations+in+Large+Vision-Language+Models+through+Hallucination-Induced+Optimization&amp;scisig=AFWwaeaHmXVMl_CTTxu5IVbaGU3N" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2405.15356&amp;rt=Alleviating+Hallucinations+in+Large+Vision-Language+Models+through+Hallucination-Induced+Optimization&amp;scisig=AFWwaeaHmXVMl_CTTxu5IVbaGU3N" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DjhKbnNhwhc&amp;hl=en&amp;sa=X&amp;d=7883897504853474460&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeb2-3PUT9p3jqqYqSpb6k6a&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=7&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</a></h3><div style="color:#006621;line-height:18px">X Wu, Z Peng, KSR Sai, HT Wu, Y Fang\xa0- The Second Workshop on Generative Information\xa0…</div><div class="gse_alrt_sni" style="line-height:17px">Effective passage retrieval and reranking methods have been widely utilized to <br>identify suitable candidates in open-domain question answering tasks, recent studies <br>have resorted to LLMs for reranking the retrieved passages by the log-likelihood of\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=nDxhjgI7aW0J&amp;citsig=AIIUsnMAAAAAaEIZ0iki7zM-y4QxEFT_Uat5irg" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://openreview.net/pdf%3Fid%3DjhKbnNhwhc&amp;rt=Passage-specific+Prompt+Tuning+for+Passage+Reranking+in+Question+Answering+with+Large+Language+Models&amp;scisig=AFWwaeYEgxK8TXCdcse5i0uq-1_k" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://openreview.net/pdf%3Fid%3DjhKbnNhwhc&amp;rt=Passage-specific+Prompt+Tuning+for+Passage+Reranking+in+Question+Answering+with+Large+Language+Models&amp;scisig=AFWwaeYEgxK8TXCdcse5i0uq-1_k" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://openreview.net/pdf%3Fid%3DjhKbnNhwhc&amp;rt=Passage-specific+Prompt+Tuning+for+Passage+Reranking+in+Question+Answering+with+Large+Language+Models&amp;scisig=AFWwaeYEgxK8TXCdcse5i0uq-1_k" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2405.19315&amp;hl=en&amp;sa=X&amp;d=14177291864252939115&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeZpDtMRxzghFvETidLqjsv9&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=8&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Matryoshka Query Transformer for Large Vision-Language Models</a></h3><div style="color:#006621;line-height:18px">W Hu, ZY Dou, LH Li, A Kamath, N Peng, KW Chang\xa0- arXiv preprint arXiv\xa0…, 2024</div><div class="gse_alrt_sni" style="line-height:17px">Large Vision-Language Models (LVLMs) typically encode an image into a fixed <br>number of visual tokens (eg, 576) and process these tokens with a language model. <br>Despite their strong performance, LVLMs face challenges in adapting to varying\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=a_vUBdbbv8QJ&amp;citsig=AIIUsnMAAAAAaEIZ0t-oOZmFkQNDT-xsodcqAG0" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2405.19315&amp;rt=Matryoshka+Query+Transformer+for+Large+Vision-Language+Models&amp;scisig=AFWwaebqgfJfnUZ3aq-AKeLFBY2A" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2405.19315&amp;rt=Matryoshka+Query+Transformer+for+Large+Vision-Language+Models&amp;scisig=AFWwaebqgfJfnUZ3aq-AKeLFBY2A" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2405.19315&amp;rt=Matryoshka+Query+Transformer+for+Large+Vision-Language+Models&amp;scisig=AFWwaebqgfJfnUZ3aq-AKeLFBY2A" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:11px;font-weight:bold;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2405.14622&amp;hl=en&amp;sa=X&amp;d=15461348115330694510&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeaI5PF79pOwZF1c5DFOcSmC&amp;oi=scholaralrt&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;pos=9&amp;folt=rel" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Calibrated Self-Rewarding Vision Language Models</a></h3><div style="color:#006621;line-height:18px">Y Zhou, Z Fan, D Cheng, S Yang, Z Chen, C Cui…\xa0- arXiv preprint arXiv\xa0…, 2024</div><div class="gse_alrt_sni" style="line-height:17px">Large Vision-Language Models (LVLMs) have made substantial progress by <br>integrating pre-trained large language models (LLMs) and vision models through <br>instruction tuning. Despite these advancements, LVLMs often exhibit the\xa0…</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=en&amp;update_op=email_library_add&amp;info=bv3wcjG-kdYJ&amp;citsig=AIIUsnMAAAAAaEIZ0tx2xa-CVKynhGAZssVQsnI" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="Save" src="https://scholar.google.com/intl/en/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2405.14622&amp;rt=Calibrated+Self-Rewarding+Vision+Language+Models&amp;scisig=AFWwaeYAo0e23CeBqx_UkWvh3Mkk" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/en/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2405.14622&amp;rt=Calibrated+Self-Rewarding+Vision+Language+Models&amp;scisig=AFWwaeYAo0e23CeBqx_UkWvh3Mkk" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/en/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=en&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2405.14622&amp;rt=Calibrated+Self-Rewarding+Vision+Language+Models&amp;scisig=AFWwaeYAo0e23CeBqx_UkWvh3Mkk" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/en/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><div style="line-height:16px;mso-line-height-rule:exactly;border-top:1px solid #bdbdbd">&nbsp;</div><p style="margin:8px 0 16px 0;color:#666">This message was sent by Google Scholar because you\'re following new articles related to research by <a href="https://scholar.google.com/citations?hl=en&amp;user=8rDNIMsAAAAJ" style="color:#1a0dab;">Scott Wen-tau Yih</a>.<img src="https://scholar.google.com/scholar_url?url=https://scholar.google.com/scholar/images/cleardot.gif&amp;hl=en&amp;sa=X&amp;ei=UuZgZt-TNY3A6rQP6Kmr0AI&amp;scisig=AFWwaeaD7BBaOOzddxOwf5MdddkY&amp;hist=3srgq2MAAAAJ:7195817972327493252:AFWwaeaqD-kAKcmcQcPJtIGAQqB3&amp;html=&amp;folt=rel&amp;lrrs=0,1,2,3,4,5,6,7,8,9" height=1 width=1 alt=""></p><div style="margin-bottom:8px;"><div><!--[if gte mso 9]><table border="0" cellspacing="0" cellpadding="0"><tr><td style="mso-line-height-rule:exactly;line-height:27px;border-top:1px solid #fff;border-bottom:1px solid #fff;mso-text-raise:-1px;"><![endif]--><a href="https://scholar.google.com/scholar_alerts?view_op=cancel_alert_options&amp;email_for_op=rich%40richjames.co&amp;alert_id=hK7zHFKu3GMJ&amp;hl=en&amp;citsig=AIIUsnMAAAAAZnNbUjry2kphgP90dSZe-F2WUQ0" style="display:inline-block;text-decoration:none;font-family:arial,sans-serif;font-size:13px;font-size:11px;text-transform:uppercase;font-size:13px;line-height:21px;padding:3px 0;color:#1a0dab;border-top:1px solid transparent;border-bottom:1px solid transparent;border-radius:3px;mso-padding-alt:0;mso-border-alt:none;"><span style="mso-text-raise:5px">Cancel alert</span></a><!--[if gte mso 9]></td></tr></table><![endif]--></div></div></div><!--[if gte mso 9]></td></tr></table><![endif]--></body></html>\r\n'